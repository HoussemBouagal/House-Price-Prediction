
# ğŸ  House Price Prediction with Machine Learning

This project is focused on predicting house prices using various regression models. It was developed as a solution for the Kaggle competition: [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).

## ğŸš€ Project Overview

We built a machine learning pipeline to process data, train models, evaluate performance, and generate predictions for submission. The target variable is the sale price of houses, and the goal is to minimize the RMSE (Root Mean Squared Error) between predicted and actual values.

## ğŸ“Š Models Used

- **Lasso Regression**
- **Random Forest Regressor**
- **XGBoost Regressor** âœ… Best Performance

## ğŸ“ˆ Results

| Model               | RMSE    |
|--------------------|---------|
| Lasso Regression   | 0.1383  |
| Random Forest      | 0.1462  |
| XGBoost Regressor  | 0.1292 âœ… |

The XGBoost model was the most accurate and was used to generate the final predictions.

## ğŸ”§ Tools & Libraries

- Python
- Pandas & NumPy
- Scikit-learn
- XGBoost
- Seaborn & Matplotlib
- Joblib

## ğŸ“¦ Requirements & Installation

Before running the project, install the required dependencies using the following command:

```bash
pip install -r requirements.txt
```

Ensure you have Python 3.8 or later installed.

## ğŸ›  Workflow

1. **Load Data**
2. **Data Preprocessing**
   - Combine train/test datasets
   - One-hot encoding for categorical features
   - Missing value imputation using mean strategy
3. **Model Training & Validation**
   - Models trained using 80/20 train-validation split
4. **Model Evaluation**
   - RMSE calculated on validation set
   - Learning curve plotted for XGBoost
5. **Feature Importance & Residual Analysis**
6. **Submission File Generation**
7. **Model Saving & Loading for Reuse**

## ğŸ“¸ Screenshots

### ğŸ“Š Feature Importance (XGBoost)
![Feature Importance](screenshots/importance.png)

### ğŸ“‰ Residual Plot
![Residual Plot](screenshots/residual.png)

### ğŸ“š Learning Curve
![Learning Curve](screenshots/learning_curve.png)


## ğŸ Final Submission

The final predictions were submitted on Kaggle and scored a **Public Score: 0.13012**, confirming the effectiveness of the XGBoost model.

## ğŸ“‚ Directory Structure

```
 House-Price-Prediction/
â”‚
â”œâ”€â”€ submission.csv           # Final predictions for Kaggle
â”œâ”€â”€ house_prediction_model.pkl  # Trained model file
â”œâ”€â”€ requirements.txt         # Required Python packages
â”œâ”€â”€ screenshots/             # Folder for plots 
â”‚   â”œâ”€â”€ importance.png
â”‚   â”œâ”€â”€ residual.png
â”‚   â””â”€â”€ learning_curve.png
â””â”€â”€ README.md                # Project description
```

## ğŸ† Kaggle Submission

- A public score of **0.13012** was achieved using the XGBoost model.
- Earned the **Getting Started Competitor** badge on Kaggle âœ…

## ğŸ‘¨â€ğŸ’» Author

**Houssem Bouagal**  
ğŸ“§ mouhamedhoussem813@gmail.com

---
ğŸ”— Made with ğŸ’¡ and ML passion.
